---
layout: single
title: Running Spark Locally
date: 2018-01-30 08:46:39.000000000 +05:30
type: post
parent_id: '0'
published: true
password: ''
status: publish
categories:
- apache-spark
- Tutorials
tags: []
meta:
  timeline_notification: '1517282201'
  _rest_api_published: '1'
  _rest_api_client_id: "-1"
  _publicize_job_id: '14176985563'
author:
  login: sureshsarda
  email: sureshssarda@gmail.com
  display_name: Suresh Sarda
  first_name: ''
  last_name: ''
excerpt: A quick guide on setting up Apache Spark on your local machine for development
  and testing.
---
<p>Running distributed applications on local machine might seem like a tedious task, but apparently it’s pretty easy.</p>
<h3 id="1-Download-the-binary-and-extract-the-binary">1. Download and extract the binary</h3>
<p>The binary can be downloaded from <a href="https://spark.apache.org/downloads.html">here</a>, extract it somewhere, let’s call it <code>SPARK_HOME</code></p>
<h3 id="2-Running-the-master">2. Running the master</h3>
<pre class="code cm-s-dracula CodeMirror"><code class="shell"><span class="cm-builtin">cd</span> <span class="cm-def">$SPARK_HOME</span>/sbin
./start-master.sh
</code><i></i></pre>
<p><em>Verify permissions if you are not able to run it.</em><br />
The script will emit some log statement and a URL where master is running. On local machine this will be of format: <code>spark://your-machine-name:7077</code></p>
<h3 id="3-Verifying-master-run-properly">3. Verifying with Web UI</h3>
<p>Spark ships with a UI as well and it is started automatically (default settings) at port <code>8080</code>, so you can navigate to <code>localhost:8080</code> and see if things are running fine</p>
<h3 id="4-Running-Slaves">4. Running Slaves</h3>
<p>Run the slaves/workers by executing:</p>
<pre class="code cm-s-dracula CodeMirror"><span class="lineNumber CodeMirror-gutters"><span class="CodeMirror-linenumber">1</span></span><code class="shell"><span class="cm-quote">./start-slave.sh </span><span class="cm-def">$SPARK_URL</span><span class="cm-quote"> -m 512m -c 1</span>
</code><i></i></pre>
<p>The first parameter is the URL at which spark master is running.<br />
You can also provide the memory and CPU cores the worker will take, in this example its 512mb and 1 core. The default is <code>(total memory - 1gb)</code> memory and all available cores which is not recommended in local mode.</p>
<p>You can verify on the UI if the slave got attached to the master.</p>
<h3 id="5-Stopping">5. Stopping</h3>
<p>To stop, execute <code>./stop-all.sh</code> in the same directory. If things get crazy you can of course find the tasks using <code>ps -aux | grep "spark"</code> and kill them individually.</p>
<p>Detailed information can be found <a href="https://spark.apache.org/docs/latest/spark-standalone.html">here</a></p>
