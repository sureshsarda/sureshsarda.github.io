<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Notes Page on s9a.me</title><link>s9a.me/posts/</link><description>Recent content in Notes Page on s9a.me</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 02 Apr 2020 00:00:00 +0000</lastBuildDate><atom:link href="s9a.me/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Why you should not use null as a method parameter</title><link>s9a.me/posts/2020-01-06-design-oop-avoid-using-nulls-method-argument/</link><pubDate>Mon, 06 Jan 2020 00:00:00 +0000</pubDate><guid>s9a.me/posts/2020-01-06-design-oop-avoid-using-nulls-method-argument/</guid><description>Object Oriented concept was first introduced with Lisp in the late 1950s. Lisp has atoms and attributes, where the atoms represented an real world object. OOP has evolved quite a bit after that. SOLID design principle were introduced that helped us craft a better software. I will not go in details of how these principles help us, there is already lot of material out there. Instead I&amp;rsquo;ll focus on some mistakes we do while implementing these patterns.</description></item><item><title>Insert, Index document in Elasticsearch using Java</title><link>s9a.me/elasticsearch/insert-index-document-elasticsearch-java/</link><pubDate>Wed, 27 Nov 2019 00:00:00 +0000</pubDate><guid>s9a.me/elasticsearch/insert-index-document-elasticsearch-java/</guid><description>In this article we will see how to insert a document in Elasticsearch using the High Level Client provided by Elasticsearch.
Prepare the Index Request Elasticsearch client accepts IndexRequest to insert documents. Two required details for this request are:
Name of the index Document source Everything else can take default value. We will stick to the easy path to create our very first document and explore other optional arguments later.</description></item><item><title>Search Elasticsearch from Java using High Level Client</title><link>s9a.me/elasticsearch/search-match-all-from-java-using-high-level-client/</link><pubDate>Tue, 26 Nov 2019 00:00:00 +0000</pubDate><guid>s9a.me/elasticsearch/search-match-all-from-java-using-high-level-client/</guid><description>In [the last post]({% post_url elasticsearch/2019-11-25-connecting-to-elasticsearch-from-java %}) we saw how to connect to Elasticsearch using the High Level Client.
match_all Query using Java client The client provides many high level methods to build an Elasticsearch query. To keep it simple, let&amp;rsquo;s see how to construct a simple match_all query:
SearchRequest request = new SearchRequest(&amp;#34;online&amp;#34;); // -- 1 SearchSourceBuilder builder = new SearchSourceBuilder(); // -- 2 builder.query(QueryBuilders.matchAllQuery()); request.source(builder); // -- 3 Parameter to the constructor is the index you want to query, this can be left empty and all the indices will be queried SearchSourceBuilder is used to create a search request.</description></item><item><title>Connect to Elasticsearch using Java High Level Client</title><link>s9a.me/elasticsearch/connect-from-java-using-high-level-client/</link><pubDate>Mon, 25 Nov 2019 00:00:00 +0000</pubDate><guid>s9a.me/elasticsearch/connect-from-java-using-high-level-client/</guid><description>Elasticsearch provides a client to conveniently connect with Elasticsearch. Create a Maven project and add the following maven dependency:
Add Maven Dependency &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.elasticsearch.client&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;elasticsearch-rest-high-level-client&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;7.3.1&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; Set Up connection with Elasticsearch Elasticsearch maintains 2 clients - Low Level Client and High Level Client. It is recommended to use the High Level Client to query and index documents in Elasticsearch. You can create connection to Elasticsearch like this:
RestHighLevelClient client = new RestHighLevelClient( RestClient.</description></item><item><title>Create your first Docker container</title><link>s9a.me/posts/2019-09-28-creating-your-first-docker-image/</link><pubDate>Sat, 28 Sep 2019 10:00:00 +0000</pubDate><guid>s9a.me/posts/2019-09-28-creating-your-first-docker-image/</guid><description>Part 1 of a multi-part essay on configuring Docker in production.
Prerequisites You have Docker installed (See instructions to install) (Optional) You know to to deploy a Flask application using command line Docker works on basis of configurations. It&amp;rsquo;s a declarative way of telling Docker how to build images.
A typical workflow starts with a base image, works on top of it, saves it either for future use or gets reused as a base image of something else.</description></item><item><title>Pandas: Splitting (Exploding) a column into multiple rows</title><link>s9a.me/posts/2018-11-21-pandas-splitting-exploding-string-column-into-multiple-rows/</link><pubDate>Wed, 21 Nov 2018 10:00:00 +0000</pubDate><guid>s9a.me/posts/2018-11-21-pandas-splitting-exploding-string-column-into-multiple-rows/</guid><description>Recently, while working with on something in my office, I faced a small but interesting problem. I had to clean some data and the data was not normalized. In one of the columns, a single cell had multiple comma seperated values. I could not find out the distribution of how frequently the value was appearing without splitting these cells into individual cells of their own; creating new rows.
Example:
# Input data: EmployeeId, City 001, Mumbai|Bangalore 002, Pune|Mumbai|Delhi 003, Mumbai|Bangalore 004, Mumbai|Pune 005, Bangalore .</description></item><item><title>On The Turing Test</title><link>s9a.me/posts/2018-08-04-the-turing-test/</link><pubDate>Sat, 04 Aug 2018 04:47:00 +0000</pubDate><guid>s9a.me/posts/2018-08-04-the-turing-test/</guid><description>The Imitation Game or Turing Test has become the hallmark of the question - ‘Can machines think?’. Turing predicts that by the end of 20th century, machine would be able to think. Humans have been on moon after that and machines are still at the dawn of thinking. Thinking. The paper is philosophical one and ‘think’ is a vague term which cannot be determined in simple yes or no. Turing has therefore, given a method, a test through which we can assess if machines can think.</description></item><item><title>Running in Pig in Local Mode in Java</title><link>s9a.me/posts/2018-08-01-running-hadoop/</link><pubDate>Wed, 01 Aug 2018 07:11:02 +0000</pubDate><guid>s9a.me/posts/2018-08-01-running-hadoop/</guid><description>Overview of steps Create a new maven project Add Hadoop and Pig dependencies Write a small pig script to count words in a file Write a driver program that will run this pig script 1. Create a new maven project mvn archetype:generate -DgroupId=com.example -DartifactId=piglocal -DarchetypeArtifactId=maven-archetype-webapp -DinteractiveMode=false You can now import the project in your IDE.
2. Add Hadoop and Pig dependencies Pig needs the following dependencies:
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.pig&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;pig&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;0.17.0&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;log4j&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;log4j&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.</description></item><item><title>Running Spark Locally</title><link>s9a.me/posts/2018-01-30-running-spark-locally/</link><pubDate>Tue, 30 Jan 2018 00:00:00 +0000</pubDate><guid>s9a.me/posts/2018-01-30-running-spark-locally/</guid><description>Running distributed applications on local machine might seem like a tedious task, but apparently it’s pretty easy.
1. Download and extract the binary The binary can be downloaded from here, extract it somewhere, let’s call it SPARK_HOME
2. Running the master cd $SPARK_HOME/sbin ./start-master.sh Verify permissions if you are not able to run it
The script will emit some log statement and a URL where master is running. On local machine this will be of format: spark://your-machine-name:7077</description></item><item><title>Boilerplate - Logging in Python</title><link>s9a.me/posts/2017-08-03-boilerplate-for-logging-in-python/</link><pubDate>Thu, 03 Aug 2017 00:00:00 +0000</pubDate><guid>s9a.me/posts/2017-08-03-boilerplate-for-logging-in-python/</guid><description>This is how all my Python scripts look like when they start:
import logging logging.basicConfig(level=logging.DEBUG) logger = logging.getLogger(__name__) logger.debug(&amp;#39;Log message&amp;#39;) Nothing much but I prefer logging over print statements. Whenever I want to move this small script to larger project, I don&amp;rsquo;t have to make lot of changes. Logging also offers more control over the messages and what messages to print or skip.</description></item><item><title>Vagrant - Setting up and tearing down development environments on the fly</title><link>s9a.me/posts/2017-07-02-vagrant-setting-up-and-tearing-down-development-environments-on-the-fly/</link><pubDate>Sun, 02 Jul 2017 00:00:00 +0000</pubDate><guid>s9a.me/posts/2017-07-02-vagrant-setting-up-and-tearing-down-development-environments-on-the-fly/</guid><description>I try a lot of new things and this trying new things forces me to install something new. I hate installing new packages/frameworks just because I want to try it. Nothing is removed cleanly later and your OS is left with configuration files, user files, hidden directories and lot more trash created by this new package. I hate that!
Up till now, I had no option. I used to install, then remove and after a while lot of unwanted files would be lying here and there.</description></item><item><title>Elasticsearch Bulk API Example and Performance Comparison</title><link>s9a.me/posts/elasticsearch/2017-04-14-elasticsearch-bulk-api-example-and-performance-comparison/</link><pubDate>Fri, 14 Apr 2017 10:14:10 +0000</pubDate><guid>s9a.me/posts/elasticsearch/2017-04-14-elasticsearch-bulk-api-example-and-performance-comparison/</guid><description>Elasticsearch provides bulk operations to perform multiple operations in a single call. Bulk APIs can be accessed by hitting the _bulk endpoint. This post demonstrates the use of bulk API with Python. It assumes that you are familiar (not expert) with REST Bulk API of Elasticsearch. To keep it simple we will just consider the insertion case.
Problem Statement Before we jump into code, let’s take a minute and think about the problem in hand.</description></item><item><title>Memento</title><link>s9a.me/posts/2016-08-11-memento/</link><pubDate>Thu, 11 Aug 2016 00:00:00 +0000</pubDate><guid>s9a.me/posts/2016-08-11-memento/</guid><description>Ever used an image editing software? We opened an image but it’s too sharp and the light is not good. We make some changes and still not satisfied, we make few more changes only to realize that the previous changes were better than these new! We want to restore to previous changes. If we were using a drawing sheet and paint brush, this task would be very difficult; perhaps impossible. But with softwares, that’s not a problem.</description></item></channel></rss>